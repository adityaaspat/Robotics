{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPgpkFCKKhlX",
        "outputId": "3c2c8bf6-e702-429e-8235-8e00622ae400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[atari])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.4.0)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n",
            "Collecting autorom[accept-rom-license]\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=0078375623eba68944c83c04d1125ea15c5a584ee91f3cad2d4ff12a3c4e1aa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\n",
            "Collecting highway-env\n",
            "  Downloading highway_env-1.8.2-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium>=0.27 (from highway-env)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from highway-env) (1.25.2)\n",
            "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from highway-env) (2.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from highway-env) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from highway-env) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from highway-env) (1.11.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27->highway-env) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27->highway-env) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.27->highway-env)\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->highway-env) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->highway-env) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.16.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, highway-env\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 highway-env-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "!pip install highway-env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKyhtgVRKrup",
        "outputId": "5c02d69c-0acc-4c5e-8200-54bdbd7e86ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment exit-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment highway-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment highway-fast-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment intersection-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment intersection-v1 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment intersection-multi-agent-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment intersection-multi-agent-v1 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment lane-keeping-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment merge-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment parking-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment parking-ActionRepeat-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment parking-parked-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment racetrack-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment roundabout-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment two-way-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment u-turn-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        }
      ],
      "source": [
        "import highway_env\n",
        "highway_env.register_highway_envs()\n",
        "import gymnasium as gym\n",
        "import sys\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import os\n",
        "import cv2\n",
        "import torch.distributions as distributions\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qSn8W0Lfw2l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jcGMZIMUc80"
      },
      "outputs": [],
      "source": [
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.state_values = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILhP1EthUdF8"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, action_std_init=0.99):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.action_dim = action_dim\n",
        "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init)\n",
        "\n",
        "        # actor\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 512),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(512, 256),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(256, action_dim),\n",
        "                        nn.Tanh()\n",
        "                        )\n",
        "\n",
        "\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 512),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(512, 256),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(256, 1)\n",
        "                    )\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std)\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "\n",
        "        action_mean = self.actor(state)\n",
        "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state)\n",
        "\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        action_mean = self.actor(state)\n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_values, dist_entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3glGjf0M3INJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std_init=0.99):\n",
        "\n",
        "\n",
        "        self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std_init)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, action_std_init)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        self.action_std = new_action_std\n",
        "        self.policy.set_action_std(new_action_std)\n",
        "        self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "\n",
        "\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach()\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach()\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach()\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach()\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "        total_loss=0\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss+=loss\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTj-t-N55QUl",
        "outputId": "bc42cf0d-2b07-4d83-d410-c4145b6bd0cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode: 4350 | Steps: 201 | Return: -119.45 |Loss: 4737.51806640625| STD: 0.1 :  87%|████████▋ | 4351/5000 [3:30:00<29:24,  2.72s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "action_std = None\n",
        "\n",
        "rewards_list=[]\n",
        "rendered_frames=[]\n",
        "losses_list=[]\n",
        "\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "\n",
        "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\")\n",
        "env.unwrapped.config['add_walls']=False\n",
        "env.unwrapped.config['duration']=40\n",
        "\n",
        "# state space dimension\n",
        "state_dim = 6\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
        "\n",
        "lengths_list=[]\n",
        "rewards_list=[]\n",
        "losses_list=[]\n",
        "rendered_frames=[]\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "max_episodes=5000\n",
        "pbar = tqdm.trange(max_episodes)\n",
        "loss= 0.0\n",
        "flag=0\n",
        "\n",
        "for episode in pbar:\n",
        "    state,_ = env.reset(seed=0)\n",
        "    state=state['observation']\n",
        "    current_ep_reward = 0\n",
        "    step=0\n",
        "\n",
        "    # for t in range(1, max_ep_len+1):\n",
        "    for _ in range(10000):\n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, terminated ,_ = env.step(action)\n",
        "        state=state['observation']\n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        step+=1\n",
        "        time_step +=1\n",
        "        # update PPO agent\n",
        "        if time_step % 1000 == 0:\n",
        "            loss =ppo_agent.update()\n",
        "            flag=1\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if time_step % 1000 == 0:\n",
        "            ppo_agent.decay_action_std(0.98, 0.1)\n",
        "\n",
        "        if episode % 499==0:\n",
        "          rendered_frames.append(env.render())\n",
        "\n",
        "        if done or terminated:\n",
        "            break\n",
        "    # loss=ppo_agent.update()\n",
        "    if flag==1:\n",
        "      if type(loss) != float:\n",
        "        loss=loss.clone().detach().numpy().sum()\n",
        "        flag=0\n",
        "    losses_list.append(loss)\n",
        "    lengths_list.append(step)\n",
        "    rewards_list.append(current_ep_reward)\n",
        "    pbar.set_description(\n",
        "                      f'Episode: {episode} | Steps: {step + 1} | Return: {current_ep_reward:5.2f} |Loss: {loss}| STD: { ppo_agent.action_std} '\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCRKpk2cU9WS"
      },
      "outputs": [],
      "source": [
        "os.getcwd()\n",
        "os.chdir('/content/drive/MyDrive/PPO')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extlYsjgjAKg"
      },
      "outputs": [],
      "source": [
        "def moving_average(data, *, window_size = 50):\n",
        "    \"\"\"Smooths 1-D data array using a moving average.\n",
        "\n",
        "    Args:\n",
        "        data: 1-D numpy.array\n",
        "        window_size: Size of the smoothing window\n",
        "\n",
        "    Returns:\n",
        "        smooth_data: A 1-d numpy.array with the same size as data\n",
        "    \"\"\"\n",
        "    # assert data.ndim == 1\n",
        "    kernel = np.ones(window_size)\n",
        "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
        "        np.ones_like(data), kernel\n",
        "    )\n",
        "    return smooth_data[: -window_size + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STb9Yzv-jtH1"
      },
      "outputs": [],
      "source": [
        "file_path_PPO_returns = \"PPO_Returns.pkl\"\n",
        "file_path_PPO_losses=\"PPO_Losses.pkl\"\n",
        "file_path_PPO_lengths=\"PPO_Lengths.pkl\"\n",
        "file_path_PPO_frames=\"PPO_frames.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ts2Cbivj0lr"
      },
      "outputs": [],
      "source": [
        "with open(file_path_PPO_returns, 'wb') as f:\n",
        "    pickle.dump(rewards_list, f)\n",
        "with open(file_path_PPO_losses, 'wb') as f:\n",
        "    pickle.dump(losses_list, f)\n",
        "with open(file_path_PPO_frames, 'wb') as f:\n",
        "    pickle.dump(rendered_frames, f)\n",
        "with open(file_path_PPO_lengths, 'wb') as f:\n",
        "    pickle.dump(lengths_list, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NIcTF9xj3XS"
      },
      "outputs": [],
      "source": [
        "with open(file_path_PPO_returns, 'rb') as f :\n",
        "  PPO_returns = pickle.load(f)\n",
        "with open(file_path_PPO_losses, 'rb') as f :\n",
        "  PPO_losses = pickle.load(f)\n",
        "with open(file_path_PPO_frames, 'rb') as f :\n",
        "  PPO_frames = pickle.load(f)\n",
        "with open(file_path_PPO_lengths, 'rb') as f :\n",
        "  PPO_lengths = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rahWue6j8XV"
      },
      "outputs": [],
      "source": [
        "# YOUR PLOTTING CODE HERE\n",
        "plt.figure(figsize=(10, 6))\n",
        "# plt.subplot(3, 1, 1)\n",
        "plt.plot(PPO_returns, label='Returns (Raw Data)', alpha=0.5)\n",
        "plt.plot(moving_average(PPO_returns), label='Returns (Moving Average)', color='orange')\n",
        "plt.title('Returns')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Return')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(PPO_lengths, label='Lengths (Raw Data)', alpha=0.5)\n",
        "plt.plot(moving_average(PPO_lengths), label='Lengths (Moving Average)', color='orange')\n",
        "plt.title('Lengths')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Length')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(PPO_losses, label='Losses (Raw Data)')\n",
        "plt.plot(moving_average(PPO_losses), label='Losses (Moving Average)', color='orange')\n",
        "plt.title('Losses')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHWF-i5VkBR2"
      },
      "outputs": [],
      "source": [
        "frames=PPO_frames.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwwwPIKskFoe"
      },
      "outputs": [],
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')  # Choose the codec (codec may vary based on your system and installed codecs)\n",
        "fps = 25  # Specify frames per second\n",
        "frame_height, frame_width, _ = frames[0].shape  # Frame dimensions\n",
        "\n",
        "out = cv2.VideoWriter('/content/drive/My Drive/Videos/PPO_learning.mp4', fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Write frames to video\n",
        "for frame in frames:\n",
        "    out.write(frame)\n",
        "\n",
        "# Release the VideoWriter\n",
        "out.release()\n",
        "\n",
        "print(\"Video has been saved to Google Drive successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIRRESHYmHRR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}